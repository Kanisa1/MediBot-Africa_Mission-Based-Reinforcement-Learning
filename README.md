

# **Medical Drone Delivery Using Reinforcement Learning**

*A Deep Reinforcement Learning Framework for Autonomous Medical Drone Navigation in Healthcare Environments*

---

## ğŸ“Œ **Project Overview**

This project implements and compares multiple **Deep Reinforcement Learning (DRL)** algorithms to train an autonomous **medical supply delivery drone**. The environment simulates navigation tasks across three progressively complex phases:

* **Phase 1:** Simple navigation
* **Phase 2:** Obstacles + longer routes
* **Phase 3:** Constrained narrow corridors (hard)

The goal is to optimize the droneâ€™s ability to deliver supplies safely, efficiently, and reliably under varying difficulty levels.

We compare four DRL algorithms:

* **PPO** (Proximal Policy Optimization)
* **A2C** (Advantage Actor-Critic)
* **DQN** (Deep Q-Network)
* **REINFORCE** (Policy Gradient w/ Baseline)

---

## âš™ï¸ **Environment Setup**

### **State Space**

The state includes:

* Droneâ€™s current coordinates
* Target coordinates
* Distance to obstacles
* Phase difficulty indicators
* Collision risk indicators

### **Action Space**

Discrete motions:

* `Move_Forward`
* `Move_Backward`
* `Move_Left`
* `Move_Right`
* `Ascend`
* `Descend`
* `Hover`

### **Reward Function**

Reward shaping encourages:

âœ” Efficient path planning
âœ” Collision avoidance
âœ” Stable movement
âœ” Reaching the final target

Penalties include:

âœ– Collisions
âœ– Excessive path length
âœ– Unstable/jerky motion

---

# ğŸ§  **Algorithms Implemented**

## **1. DQN (Deep Q-Network)**

* Off-policy value-based method
* Uses replay buffer + target networks
* Struggles in high-dimensional continuous navigation tasks

## **2. PPO (Proximal Policy Optimization)**

* On-policy actor-critic
* Clipped surrogate objective stabilizes learning
* Best performance in all metrics

## **3. A2C (Advantage Actor-Critic)**

* Parallelized policy learning
* Faster updates but higher variance

## **4. REINFORCE (Policy Gradient)**

* Pure policy gradient
* High variance, slow convergence
* Weakest performing algorithm

---

# ğŸš€ **Implementation**

Below are all hyperparameter comparisons for transparency and reproducibility.

---

## **ğŸ”§ DQN Hyperparameters & Results**

| Run     | Learning Rate | Gamma | Replay Buffer | Batch Size | Exploration | Mean Reward | Episodes to Converge |
| ------- | ------------- | ----- | ------------- | ---------- | ----------- | ----------- | -------------------- |
| 1       | 1e-4          | 0.99  | 100K          | 32         | Îµ-greedy    | 5,847       | ~450                 |
| 2       | 1e-4          | 0.99  | 100K          | 32         | Îµ-greedy    | 6,124       | ~420                 |
| 3       | 1e-4          | 0.99  | 100K          | 32         | Îµ-greedy    | 5,932       | ~480                 |
| 4       | 1e-4          | 0.99  | 100K          | 32         | Îµ-greedy    | 6,215       | ~410                 |
| â€¦       | â€¦             | â€¦     | â€¦             | â€¦          | â€¦           | â€¦           | â€¦                    |
| **Avg** | â€”             | â€”     | â€”             | â€”          | â€”           | **â‰ˆ 6,050** | **â‰ˆ 450**            |

---

## **ğŸ”§ PPO Hyperparameters & Results**

| Run     | LR     | n_steps | Batch | Epochs | Entropy | Mean Reward | Episodes to Converge |
| ------- | ------ | ------- | ----- | ------ | ------- | ----------- | -------------------- |
| 1       | 2.5e-4 | 2048    | 128   | 15     | 0.01    | 8,234       | ~280                 |
| 2       | 2.5e-4 | 2048    | 128   | 15     | 0.01    | 8,567       | ~270                 |
| 3       | 2.5e-4 | 2048    | 128   | 15     | 0.01    | 8,012       | ~300                 |
| â€¦       | â€¦      | â€¦       | â€¦     | â€¦      | â€¦       | â€¦           | â€¦                    |
| **Avg** | â€”      | â€”       | â€”     | â€”      | â€”       | **â‰ˆ 8,500** | **â‰ˆ 275**            |

---

## **ğŸ”§ A2C Hyperparameters & Results**

| Run     | LR   | n_steps | Batch | Entropy | ValueCoef | Mean Reward | Episodes to Converge |
| ------- | ---- | ------- | ----- | ------- | --------- | ----------- | -------------------- |
| 1       | 7e-4 | 16      | 16    | 0.01    | 0.5       | 7,123       | ~350                 |
| 2       | 7e-4 | 16      | 16    | 0.01    | 0.5       | 7,456       | ~320                 |
| 3       | 7e-4 | 16      | 16    | 0.01    | 0.5       | 6,987       | ~380                 |
| â€¦       | â€¦    | â€¦       | â€¦     | â€¦       | â€¦         | â€¦           |                      |
| **Avg** | â€”    | â€”       | â€”     | â€”       | â€”         | **â‰ˆ 7,400** | **â‰ˆ 335**            |

---

## **ğŸ”§ REINFORCE Hyperparameters & Results**

| Run     | LR   | Baseline    | Entropy | Mean Reward | Episodes to Converge |
| ------- | ---- | ----------- | ------- | ----------- | -------------------- |
| 1       | 1e-3 | State Value | 0.01    | 3,456       | ~600                 |
| 2       | 1e-3 | State Value | 0.01    | 3,234       | ~650                 |
| 3       | 1e-3 | State Value | 0.01    | 3,678       | ~580                 |
| â€¦       | â€¦    | â€¦           | â€¦       | â€¦           | â€¦                    |
| **Avg** | â€”    | â€”           | â€”       | **â‰ˆ 3,500** | **â‰ˆ 600**            |

---

# ğŸ“Š **Results & Analysis**

## **1. Cumulative Reward Comparison**

| Algorithm     | Avg Reward | Rank      |
| ------------- | ---------- | --------- |
| **PPO**       | **~8,500** | ğŸ¥‡ Best   |
| **A2C**       | ~7,400     | ğŸ¥ˆ        |
| **DQN**       | ~6,050     | ğŸ¥‰        |
| **REINFORCE** | ~3,500     | âŒ Weakest |

### Key Insights

âœ” PPO produced the **highest and most stable** rewards
âœ” A2C performed well but had higher variance
âœ” DQN struggled with sample efficiency
âœ” REINFORCE suffered from high-variance gradients

---

## **2. Phase-Specific Best Rewards**

| Phase            | Best Reward | Algorithm                |
| ---------------- | ----------- | ------------------------ |
| Phase 1 â€” Easy   | **13,101**  | PPO                      |
| Phase 2 â€” Medium | **10,851**  | PPO                      |
| Phase 3 â€” Hard   | **-169.84** | All algorithms struggled |

ğŸ” **Observation:**
Phase 3â€™s difficulty spike implies curriculum learning requires more smoothing.

---

# ğŸ“ˆ **Training Stability**

## **PPO â€” Most Stable (Score: 8.5/10)**

* Smooth surrogate objective
* Healthy entropy values
* Minimal oscillations

## **A2C â€” Moderately Stable (7.5/10)**

* Occasional spikes due to small n_steps
* Stable critic in most phases

## **DQN â€” Less Stable (6.5/10)**

* TD-error oscillations
* Sensitive to Îµ-decay schedule

---

# â± **Convergence Speed**

| Algorithm | Avg Episodes to Converge | Rank       |
| --------- | ------------------------ | ---------- |
| **PPO**   | **~275**                 | ğŸ¥‡ Fastest |
| **A2C**   | ~335                     | ğŸ¥ˆ         |
| **DQN**   | ~450                     | ğŸ¥‰         |
| REINFORCE | ~600                     | âŒ Slowest  |

---

# ğŸŒ **Generalization Performance**

| Metric               | PPO       | A2C   | DQN   |
| -------------------- | --------- | ----- | ----- |
| Novel positions      | 94%       | 87%   | 79%   |
| Unseen targets       | 89%       | 84%   | 71%   |
| Cross-phase transfer | 61%       | 58%   | 44%   |
| **Overall Score**    | **81.3%** | 76.3% | 64.7% |

### Key Takeaways

âœ” PPO generalizes best
âœ” DQN struggles on unseen targets
âœ” All algorithms degrade in Phase 3 transfer

---

# ğŸ **Conclusion**

* **PPO is the optimal algorithm** for medical drone navigation in this project
* Strongest in **reward**, **stability**, **convergence**, and **generalization**
* **A2C is a strong runner-up**
* **DQN & REINFORCE are not ideal** for complex 3D navigation tasks

---

# ğŸ“¦ **Project Structure**

```
ğŸ“ medical-drone-rl
â”‚â”€â”€ ğŸ“„ README.md
â”‚â”€â”€ ğŸ“„ Medical_drone.ipynb
â”‚â”€â”€ ğŸ“ models/
â”‚â”€â”€ ğŸ“ logs/
â”‚â”€â”€ ğŸ“ results/
â”‚â”€â”€ ğŸ“„ requirements.txt
```

---

# â–¶ï¸ **How to Run**

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Open Notebook

```bash
jupyter notebook Medical_drone.ipynb
```

### 3. Train a PPO model

```python
model = PPO("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=500000)
```

---

